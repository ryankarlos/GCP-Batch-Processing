# GCP Hackathon 

My contribution towareds our hackathon team's goal of building a webapp to display best places people would like to visit as holiday using data from 
various sources.

* Building a pipeline for batch processing scrapped web data into cloud storage and big query

Scraping Booking.com hotel searches  by building on some existing work ad implementing a scraper in `scraper.py`: 
https://github.com/ZoranPandovski/BookingScraper
https://www.scrapehero.com/scrape-property-data-from-booking-com-using-google-chrome/

My first time using cloud functions in google so few steps that i spent a lot of time figuring out before I could get it working.
Used source repositories to sync by repo from github as i was developing locally. 


`main.py` contains two google cloud functions: 

- bookingsscrapper which has HTTP trigger and calls the scraper function which stores it as csv in tmp dir in the cloud vm and then 
uploads the data into google cloud storage bucket. 
- load_bookings_from_bucket which has Bucket trigger and uploads data into bigquery when new data lands in cloud storage bucket from
bookingscrapper.

The following environment variable were set by default which correspond to search parameters which can be changed before triggering.

* Calling CLoud Natural Language API to generate sentiment for text

Tweets generated by tweepy were already streamed into Bigquery via pubsub. Cloud function `senti` in `main.py`
generates

Using https://cloud.google.com/natural-language/docs/sentiment-tutorial as a baseline, the code was adjusted to
create bigquery table (after deleting exisitng one, if already exists)

