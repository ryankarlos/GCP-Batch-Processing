# GCP Batch Processing

* Building a pipeline for batch processing scrapped web data into cloud storage and big query

Scraping Booking.com hotel searches  by building on some existing work ad implementing a scraper in `scraper.py`: 
https://github.com/ZoranPandovski/BookingScraper
https://www.scrapehero.com/scrape-property-data-from-booking-com-using-google-chrome/

a) bookingsscrapper generates HTTP end point and gets triggered to run the scraper and store it as csv in tmp dir in the cloud vm and then 
uploads the data into google cloud storage bucket.
b) dataprep dataflow job which cleans the raw data landing from a) in the cloud bucket and uploads to another bucket. This can be setup as schedule
or another cloud function to trigger when listening to bucket event.
c) load_bookings_from_bucket which has Bucket trigger and uploads data into bigquery when new data lands in cloud storage bucket 

`main.py` contains all google cloud function scripts 

Note that cloud functions have a max timeout limit of 9 minutes althout default setting is 60 secs, and i had to increase this to avoid timing out, especially when increasing the number of pages I was making calls from. Any changes to source code - requires cloud function to be deployed before triggering, for the changes to take effect. The following environment variable were set by default which  correspond to search parameters which can be changed before triggering.

* Calling CLoud Natural Language API to generate sentiment for text

Tweets generated by tweepy were already streamed into Bigquery via pubsub. Cloud function `senti` in `main.py`
generates

Using https://cloud.google.com/natural-language/docs/sentiment-tutorial as a baseline, the code was adjusted to
create bigquery table (after deleting exisitng one, if already exists)

